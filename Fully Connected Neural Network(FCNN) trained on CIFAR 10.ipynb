{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f722319-c9a5-4a76-89ae-f6ea84d3533d",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network(FCNN) trained on CIFAR 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e466827-673f-4959-b605-877b0569a660",
   "metadata": {},
   "source": [
    "## CIFAR-10 Dataset Overview:\n",
    "\n",
    "- Number of Classes: 10 (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)\n",
    "- Number of Training Samples: 50,000\n",
    "- Number of Test Samples: 10,000\n",
    "- Image Size: 32x32 pixels (3 color channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d23b9-e028-4792-b1b5-309094a2a0e2",
   "metadata": {},
   "source": [
    "## Step 1: Load CIFAR-10 Dataset\n",
    "\n",
    "To download and load the CIFAR-10 dataset, we'll use the `urllib` module and extract the data from the downloaded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c618e8-f58c-49bc-b057-69d6df69fa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (50000, 32, 32, 3)\n",
      "Test data shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "import os  # Import os module here\n",
    "import shutil  # Import shutil module here\n",
    "\n",
    "# Download and extract the CIFAR-10 dataset\n",
    "def download_and_extract_cifar10():\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    file_path = \"cifar-10-python.tar.gz\"\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"Downloading CIFAR-10 dataset...\")\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "    \n",
    "    if not os.path.exists('cifar-10-batches-py'):\n",
    "        print(\"Extracting CIFAR-10 dataset...\")\n",
    "        with tarfile.open(file_path) as tar:\n",
    "            tar.extractall()\n",
    "\n",
    "download_and_extract_cifar10()\n",
    "\n",
    "# Function to load CIFAR-10 data\n",
    "def load_cifar10_batch(batch_id):\n",
    "    with open(f'cifar-10-batches-py/data_batch_{batch_id}', mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = np.array(batch['labels'])\n",
    "    return features, labels\n",
    "\n",
    "# Load all the training batches\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    features, labels = load_cifar10_batch(i)\n",
    "    X_train.append(features)\n",
    "    y_train.append(labels)\n",
    "\n",
    "X_train = np.concatenate(X_train)\n",
    "y_train = np.concatenate(y_train)\n",
    "\n",
    "# Load the test batch\n",
    "def load_cifar10_test():\n",
    "    with open('cifar-10-batches-py/test_batch', mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = np.array(batch['labels'])\n",
    "    return features, labels\n",
    "\n",
    "X_test, y_test = load_cifar10_test()\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a38d24-ecc6-441a-b077-6ede4637fb21",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess the Data\n",
    "\n",
    "Before feeding the data into the neural network, we'll normalize the pixel values (from 0-255 to 0-1) and flatten the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba3dfb94-bebe-4adb-aae2-ceaf3ed06cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Flatten the images into vectors of 3072 elements (32*32*3)\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "def one_hot_encode(y, num_classes):\n",
    "    one_hot = np.zeros((y.size, num_classes))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_train = one_hot_encode(y_train, 10)\n",
    "y_test = one_hot_encode(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e15a49e-39fa-43eb-980c-46364aa426e7",
   "metadata": {},
   "source": [
    "## Simple Fully Connected Neural Network for CIFAR-10\n",
    "\n",
    "### 1. Network Architecture\n",
    "- **Input Layer**: Each CIFAR-10 image is 32x32 pixels with 3 color channels (RGB), so the input has 3072 features (32 × 32 × 3).\n",
    "- **Hidden Layer**: There is a single hidden layer with 64 neurons. The activation function used is **sigmoid**, and **reLU** on the second network.\n",
    "- **Output Layer**: The output layer has 10 neurons, one for each class in CIFAR-10. We use the **softmax** activation function to convert the output into class probabilities.\n",
    "\n",
    "### 2. Forward Pass\n",
    "- The input data (image pixels) is passed through the network:\n",
    "  - **Input to Hidden Layer**: Each neuron in the hidden layer computes a weighted sum of its inputs, adds a bias, and applies the **sigmoid activation function**:\n",
    "    $$\n",
    "    Z_1 = W_1 \\cdot X + b_1\n",
    "    $$\n",
    "    $$\n",
    "    A_1 = \\sigma(Z_1) = \\frac{1}{1 + e^{-Z_1}}\n",
    "    $$\n",
    "  - **Hidden to Output Layer**: The output layer computes a weighted sum of the hidden layer outputs, adds biases, and applies the **softmax function**:\n",
    "    $$\n",
    "    Z_2 = W_2 \\cdot A_1 + b_2\n",
    "    $$\n",
    "    $$\n",
    "    A_2 = \\text{softmax}(Z_2) = \\frac{e^{Z_2}}{\\sum_{j} e^{Z_{2,j}}}\n",
    "    $$\n",
    "\n",
    "### 3. Prediction\n",
    "- The output is a probability distribution over the 10 classes. The class with the highest probability is the predicted class.\n",
    "\n",
    "### 4. Loss Function\n",
    "- We use **cross-entropy loss** to measure the difference between predicted probabilities and actual class labels:\n",
    "  $$\n",
    "  \\text{Loss} = - \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{c=1}^{C} y_{i,c} \\cdot \\log(\\hat{y}_{i,c})\n",
    "  $$\n",
    "  where:\n",
    "  - $( m $) is the number of training samples,\n",
    "  - $( C $) is the number of classes (10 for CIFAR-10),\n",
    "  - $( y_{i,c} $) is the true label for sample $( i $) and class $( c $),\n",
    "  - $( \\hat{y}_{i,c} $) is the predicted probability for sample $( i $) and class $( c $).\n",
    "\n",
    "### 5. Backpropagation (Gradient Descent)\n",
    "- The model computes gradients (errors) for both the hidden layer and output layer, and updates the weights using **gradient descent**:\n",
    "  - For the weights and biases between the hidden and output layers:\n",
    "    $$\n",
    "    W_2 = W_2 - \\text{learning rate} \\times \\frac{\\partial \\text{Loss}}{\\partial W_2}\n",
    "    $$\n",
    "    $$\n",
    "    b_2 = b_2 - \\text{learning rate} \\times \\frac{\\partial \\text{Loss}}{\\partial b_2}\n",
    "    $$\n",
    "  - For the weights and biases between the input and hidden layers:\n",
    "    $$\n",
    "    W_1 = W_1 - \\text{learning rate} \\times \\frac{\\partial \\text{Loss}}{\\partial W_1}\n",
    "    $$\n",
    "    $$\n",
    "    b_1 = b_1 - \\text{learning rate} \\times \\frac{\\partial \\text{Loss}}{\\partial b_1}\n",
    "    $$\n",
    "\n",
    "### 6. Training\n",
    "- The network is trained over multiple **epochs**. In each epoch:\n",
    "  1. Perform a **forward pass** to compute the output.\n",
    "  2. Compute the **loss** using cross-entropy.\n",
    "  3. Perform **backpropagation** to adjust the weights and biases to minimize the loss.\n",
    "\n",
    "### 7. Evaluation (Accuracy)\n",
    "- After training, the model is tested on the test set. **Accuracy** is calculated as the percentage of correctly classified images:\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Test Images}} \\times 100\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2008ae3a-d27b-4122-bc3c-9f314dcab597",
   "metadata": {},
   "source": [
    "# Simple Neural Network with Sigmoid\n",
    "\n",
    "$$\n",
    "\\begin{array}{cccc}\n",
    "\\text{Input Layer} & \\longrightarrow & \\text{Hidden Layer} & \\longrightarrow & \\text{Output Layer} \\\\\n",
    "\\left(3072\\right)  &                  & \\left(64\\right)     &                  & \\left(10\\right) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf9468-f965-4b19-8e21-2408f22b73dd",
   "metadata": {},
   "source": [
    "# Step 1: Initialization (`__init__`)\n",
    "\n",
    "In this step, we need to initialize:\n",
    "\n",
    "Weights: These connect the neurons between the input layer, hidden layer, and output layer. You’ll need two weight matrices:\n",
    "- `W1`: connects the input layer to the hidden layer (dimensions: input_size x hidden_size).\n",
    "- `W2`: connects the hidden layer to the output layer (dimensions: hidden_size x output_size).\n",
    "Biases: Bias vectors for each layer:\n",
    "- `b1`: for the hidden layer (dimensions: `1 x hidden_size`).\n",
    "- `b2`: for the output layer (dimensions: `1 x output_size`).\n",
    "\n",
    "The weights are initialized with small random values, typically scaled by a small factor (e.g., `*0.01`), so that the network can learn meaningful patterns. Initializing them too large can cause issues with learning, and setting them all to zero would prevent the network from learning properly (since all gradients would be identical).\n",
    "\n",
    "The biases are typically initialized to zeros.\n",
    "\n",
    "## Why Initialize Like This?\n",
    "\n",
    "In backpropagation, if weights are too large, they can cause the gradients to become extremely large or extremely small during updates, leading to slow or poor learning (this is the problem of vanishing or exploding gradients). By scaling the random initialization, we reduce the likelihood of this happening, ensuring that gradients stay at reasonable values in early training.\n",
    "\n",
    "## Why Initialize Bias to Zero?\n",
    "\n",
    "Role of Bias: The bias term allows the activation of neurons to shift. It acts as an additional parameter that enables the model to fit the data more flexibly. Mathematically, a neuron computes:\n",
    "\n",
    "$ z=W⋅X+bz=W⋅X+b$  \n",
    "\n",
    "Here, `b` is the bias. Without the bias, the neuron would always pass through the origin (0,0), limiting its ability to fit data effectively. Bias helps the neuron to make decisions independent of the input by introducing a constant shift.\n",
    "\n",
    "### Why Zero Initialization for Bias:\n",
    "\n",
    "Bias does not affect symmetry breaking: Unlike weights, biases don’t impact the symmetry breaking during learning. Symmetry breaking refers to how each neuron needs to learn a unique function, which requires weights to be initialized randomly so that different neurons start off differently. Since biases don't control interactions between neurons but rather shift the activations, initializing them to zero doesn’t affect this.\n",
    "\n",
    "Gradients for Biases: During backpropagation, the gradient for the bias is simply the sum of the gradients from the next layer, so initializing it to zero doesn’t cause issues like symmetry lock (which can happen with zero-initialized weights).\n",
    "\n",
    "Alternatives to Zero Initialization: While initializing biases to zero is a common practice and works well in most cases, sometimes small random values are used, especially for deep networks. The idea is that starting with a slight bias may speed up convergence for some networks, but zero is the default for simplicity and effectiveness in most networks.\n",
    "\n",
    "In summary, biases are initialized to zero because:\n",
    "\n",
    "- They don’t break symmetry, so initializing them to zero is safe.\n",
    "- They are updated via gradient descent like weights, and zero initialization doesn’t cause any learning issues.\n",
    "\n",
    "## Weights as Matrices\n",
    "\n",
    "In a fully connected neural network, each neuron in one layer is connected to every neuron in the next layer. The weights determine the strength of these connections.\n",
    "\n",
    "For this network, there are:\n",
    "\n",
    "## Input Layer to Hidden Layer Weights (`W1`):\n",
    "- There are `input_size` neurons in the input layer (in this case, 3072 neurons for the 32x32 RGB images).\n",
    "- There are `hidden_size` neurons in the hidden layer (in this case, 64 neurons).\n",
    "\n",
    "    Therefore, the weight matrix W1 connects these two layers, and its dimensions will be:\n",
    "    $ W1∈R_{input\\_size×hidden\\_size}$\n",
    "\n",
    "    In this case: $W1∈R_{3072×64}$\n",
    "\n",
    "    Each element of this matrix corresponds to the weight of the connection between one input neuron and one hidden layer neuron.\n",
    "\n",
    "    ## Hidden Layer to Output Layer Weights (`W2`):\n",
    "- `hidden_size` neurons in the hidden layer (64 neurons).\n",
    "- `output_size` neurons in the output layer (10 neurons, since you’re doing classification into 10 categories).\n",
    "\n",
    "    The weight matrix `W2` will connect these two layers, and its dimensions will be:\n",
    "    $W2∈R_{hidden\\_size×output\\_size}$\n",
    "\n",
    "    In this case: $W2∈R_{64×10}$\n",
    "\n",
    "Each row of the weight matrix corresponds to a different neuron in the previous layer, and each column corresponds to a different neuron in the next layer.\n",
    "Why Matrices?\n",
    "\n",
    "Matrix multiplication allows us to compute the output of all neurons in the next layer simultaneously. When we multiply the input vector by the weight matrix, we effectively compute the weighted sum of all inputs for every neuron in the next layer at once.\n",
    "\n",
    "So in summary, the weights are matrices where:\n",
    "\n",
    "- W1 connects the input layer to the hidden layer (3072 x 64).\n",
    "- W2 connects the hidden layer to the output layer (64 x 10).\n",
    "## Why 3072 Neurons in First Layer\n",
    "Step-by-Step Breakdown:\n",
    "\n",
    "### Image Size (32x32):\n",
    "- The image size is 32×32 pixels.\n",
    "- For a grayscale image, each pixel would be represented by a single intensity value (one channel).\n",
    "\n",
    "###  RGB Channels:\n",
    "- For an RGB image, each pixel is represented by three color channels (Red, Green, Blue), which means each pixel has three values.\n",
    "- So, for each pixel, there are three values: one for red, one for green, and one for blue.\n",
    "\n",
    "### Total Number of Inputs:\n",
    "- The total number of pixels in a 32x32 image is:\n",
    "- 32×32=1024 pixels\n",
    "- Since each pixel has 3 color channels (RGB), the total number of input values (features) is:\n",
    "- 1024×3=3072\n",
    "This means the input layer of the neural network has 3072 neurons, one for each input value (R, G, and B for each pixel).\n",
    "\n",
    "### Why Flatten?\n",
    "\n",
    "Most basic neural networks (before convolutional layers) take input as a 1D vector, so the 32x32x3 image is flattened into a 1D array with 3072 elements. Each element in this array corresponds to the intensity of one color channel in one pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9995225-9506-4524-95e2-3f29637e6753",
   "metadata": {},
   "source": [
    "# Sigmoid Activation Function\n",
    "\n",
    "The sigmoid function is a smooth, differentiable function that outputs values between 0 and 1. It's commonly used in the output layer for binary classification, but it can also be used in hidden layers for learning non-linear patterns.\n",
    "\n",
    "## Mathematical Definition:\n",
    "The sigmoid function is defined as:\n",
    "$$\n",
    "\\text{Sigmoid}(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "Where:\n",
    "- $ z $ is the input to the neuron (i.e., the weighted sum of inputs plus bias).\n",
    "- $ e $ is the base of the natural logarithm.\n",
    "\n",
    "The sigmoid function \"squashes\" the input to a range between 0 and 1, making it ideal for probability-based outputs (for instance, in binary classification problems).\n",
    "\n",
    "## Derivative of the Sigmoid Function:\n",
    "The derivative of the sigmoid function (used in backpropagation) is:\n",
    "$$\n",
    "\\text{Sigmoid}'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))\n",
    "$$\n",
    "Where $ \\sigma(z) $ is the sigmoid of $ z $. This derivative will be useful during the backward pass to update the weights.\n",
    "\n",
    "## When to Use Sigmoid:\n",
    "- **Hidden Layers**: Sigmoid can be used in hidden layers, but ReLU has largely replaced it due to the **vanishing gradient problem**.\n",
    "- **Output Layer**: Sigmoid is commonly used in the output layer when performing **binary classification** (i.e., when predicting probabilities of two classes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5eab52-0f1d-495d-876e-e17befa2ba9d",
   "metadata": {},
   "source": [
    "# Softmax Function\n",
    "\n",
    "The softmax function is used in the output layer of a neural network for **multi-class classification**. It converts raw scores (logits) into probabilities, ensuring that the sum of the output probabilities equals 1. This is useful when predicting which class an input belongs to.\n",
    "\n",
    "## Mathematical Definition:\n",
    "The softmax function for a given input $ z_i $ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ z_i $ is the input to the softmax function (logit) for class $ i $.\n",
    "- $ e $ is the base of the natural logarithm.\n",
    "- The denominator $ \\sum_{j} e^{z_j} $ is the sum of the exponentials of all the logits, ensuring the outputs sum to 1.\n",
    "\n",
    "## Explanation:\n",
    "- The softmax function transforms the logits into a probability distribution, where each element represents the probability of the input belonging to a particular class.\n",
    "- The exponentiation $ e^{z_i} $ ensures that all values are positive.\n",
    "- Dividing by the sum of all exponentials normalizes the values, so that they sum to 1.\n",
    "\n",
    "## Example:\n",
    "If there are three classes and the logits are $ z = [1.0, 2.0, 0.5] $, applying the softmax function will output probabilities such as $ [0.211, 0.576, 0.213] $, which sum to 1.\n",
    "\n",
    "## Adjustment for Numerical Stability:\n",
    "\n",
    "When working with the softmax function, exponentiating large values in `z` can lead to overflow, causing numerical issues. To avoid this, before calculating the exponentials, subtract the maximum value in `z` from every element in `z`. This won’t affect the output of the softmax function but will prevent potential overflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7424ccb-c64e-4dee-bc9d-c8a9834e5fb3",
   "metadata": {},
   "source": [
    "# Forward Pass \n",
    "\n",
    "The forward pass involves calculating the activations for each layer of the neural network. Here's the step-by-step breakdown:\n",
    "\n",
    "1. **Weighted Input to the Hidden Layer**:\n",
    "   $$\n",
    "   Z_1 = X \\cdot W_1 + b_1\n",
    "   $$\n",
    "   Where:\n",
    "   - $ X $ is the input to the network.\n",
    "   - $ W_1 $ is the weight matrix connecting the input layer to the hidden layer.\n",
    "   - $ b_1 $ is the bias vector for the hidden layer.\n",
    "   - $ Z_1 $ is the linear combination of inputs, weights, and biases at the hidden layer.\n",
    "\n",
    "2. **Activation of the Hidden Layer** (using the sigmoid function):\n",
    "   $$\n",
    "   A_1 = \\text{Sigmoid}(Z_1) = \\frac{1}{1 + e^{-Z_1}}\n",
    "   $$\n",
    "   Where $ A_1 $ is the activation of the hidden layer after applying the sigmoid activation function.\n",
    "\n",
    "3. **Weighted Input to the Output Layer**:\n",
    "   $$\n",
    "   Z_2 = A_1 \\cdot W_2 + b_2\n",
    "   $$\n",
    "   Where:\n",
    "   - $ A_1 $ is the activation from the hidden layer.\n",
    "   - $ W_2 $ is the weight matrix connecting the hidden layer to the output layer.\n",
    "   - $ b_2 $ is the bias vector for the output layer.\n",
    "   - $ Z_2 $ is the linear combination of inputs, weights, and biases at the output layer.\n",
    "\n",
    "4. **Activation of the Output Layer** (using the softmax function):\n",
    "   $$\n",
    "   A_2 = \\text{Softmax}(Z_2) \n",
    "   $$\n",
    "   Where $ A_2 $ is the final output of the network, after applying the softmax activation function. This gives the final output probabilities, which will sum to 1.\n",
    "\n",
    "---\n",
    "\n",
    "This forward pass computes the activations of each layer and ultimately the output of the neural network. The sigmoid function is used to introduce non-linearity at each layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b0b99-9d57-4724-8603-f44fb42b188b",
   "metadata": {},
   "source": [
    "# Backward Pass Implementation\n",
    "\n",
    "The backward pass is responsible for calculating the gradients of the loss with respect to the network's weights and biases and updating them to minimize the loss using backpropagation. Here's the step-by-step breakdown:\n",
    "\n",
    "1. **Output Layer Error**:\n",
    "   - Compute the error at the output layer, which is the difference between the predicted output (`A2`, stored as `output`) and the true labels (`y`):\n",
    "   $$\n",
    "   \\text{error\\_output} = A2 - y\n",
    "   $$\n",
    "   This gives the error for each class in the final output.\n",
    "\n",
    "2. **Gradient for `W2` and `b2` (Output Layer Weights and Biases)**:\n",
    "   - The gradient of the loss with respect to the weights connecting the hidden layer to the output layer (`W2`) is computed as:\n",
    "   $$\n",
    "   dW2 = A1^T \\cdot \\text{error\\_output}\n",
    "   $$\n",
    "   - The gradient of the loss with respect to the bias for the output layer (`b2`) is:\n",
    "   $$\n",
    "   db2 = \\sum \\text{error\\_output}\n",
    "   $$\n",
    "   These gradients will be used to update the weights and biases for the output layer.\n",
    "\n",
    "3. **Hidden Layer Error**:\n",
    "   - Backpropagate the error to the hidden layer using the weights of the second layer (`W2`) and the derivative of the activation function (sigmoid in this case):\n",
    "   $$\n",
    "   \\text{error\\_hidden} = (\\text{error\\_output} \\cdot W2^T) \\cdot \\text{sigmoid\\_derivative}(Z1)\n",
    "   $$\n",
    "   This computes the error for the hidden layer neurons.\n",
    "\n",
    "4. **Gradient for `W1` and `b1` (Hidden Layer Weights and Biases)**:\n",
    "   - The gradient of the loss with respect to the weights connecting the input to the hidden layer (`W1`) is:\n",
    "   $$\n",
    "   dW1 = X^T \\cdot \\text{error\\_hidden}\n",
    "   $$\n",
    "   - The gradient of the loss with respect to the bias for the hidden layer (`b1`) is:\n",
    "   $$\n",
    "   db1 = \\sum \\text{error\\_hidden}\n",
    "   $$\n",
    "\n",
    "5. **Update Weights and Biases**:\n",
    "   - Use the computed gradients to update the weights and biases using gradient descent:\n",
    "   $$\n",
    "   W1 = W1 - \\text{learning\\_rate} \\cdot dW1\n",
    "   $$\n",
    "   $$\n",
    "   b1 = b1 - \\text{learning\\_rate} \\cdot db1\n",
    "   $$\n",
    "   $$\n",
    "   W2 = W2 - \\text{learning\\_rate} \\cdot dW2\n",
    "   $$\n",
    "   $$\n",
    "   b2 = b2 - \\text{learning\\_rate} \\cdot db2\n",
    "   $$\n",
    "\n",
    "This completes the backpropagation process by adjusting the weights and biases to minimize the loss.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3ee56-8637-4a42-b0ab-89acf637d7ac",
   "metadata": {},
   "source": [
    "# Backward Pass: Mathematical Breakdown\n",
    "\n",
    "The goal of the backward pass is to compute the gradients of the loss function with respect to the weights and biases and then update these parameters using gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Output Layer Error**:\n",
    "The error at the output layer is the difference between the predicted values and the true labels:\n",
    "$$\n",
    "\\text{error\\_output} = \\hat{y} - y\n",
    "$$\n",
    "Where:\n",
    "- $ \\hat{y} $ is the predicted output (from the softmax function).\n",
    "- $ y $ is the true label.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Gradient of the Loss with Respect to $ W_2 $ (Output Layer Weights)**:\n",
    "The weight gradient for $ W_2 $ is the dot product of the hidden layer activations $ A_1 $ (transpose) and the output error:\n",
    "$$\n",
    "dW_2 = A_1^T \\cdot \\text{error\\_output}\n",
    "$$\n",
    "Where:\n",
    "- $ A_1^T $ is the transpose of the activations from the hidden layer.\n",
    "- The gradient needs to be averaged over the batch size $ m $ to prevent gradient updates from being too large:\n",
    "$$\n",
    "dW_2 = \\frac{1}{m} A_1^T \\cdot \\text{error\\_output}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Gradient of the Loss with Respect to $ b_2 $ (Output Layer Bias)**:\n",
    "The bias gradient for $ b_2 $ is the sum of the output error across all examples in the batch:\n",
    "$$\n",
    "db_2 = \\sum \\text{error\\_output}\n",
    "$$\n",
    "And similarly, this gradient is averaged over the batch size $ m $:\n",
    "$$\n",
    "db_2 = \\frac{1}{m} \\sum \\text{error\\_output}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Backpropagate the Error to the Hidden Layer**:\n",
    "The error at the hidden layer is computed by backpropagating the output error using the weights $ W_2 $ and the derivative of the activation function (sigmoid) for the hidden layer:\n",
    "$$\n",
    "\\text{error\\_hidden} = (\\text{error\\_output} \\cdot W_2^T) \\cdot \\sigma'(Z_1)\n",
    "$$\n",
    "Where:\n",
    "- $ W_2^T $ is the transpose of the weights connecting the hidden layer to the output layer.\n",
    "- $ \\sigma'(Z_1) $ is the derivative of the sigmoid function applied to $ Z_1 $ (the pre-activation values from the hidden layer).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Gradient of the Loss with Respect to $ W_1 $ (Hidden Layer Weights)**:\n",
    "The weight gradient for $ W_1 $ is computed similarly:\n",
    "$$\n",
    "dW_1 = X^T \\cdot \\text{error\\_hidden}\n",
    "$$\n",
    "Where:\n",
    "- $ X^T $ is the transpose of the input data.\n",
    "- Like before, this gradient is averaged over the batch size $ m $:\n",
    "$$\n",
    "dW_1 = \\frac{1}{m} X^T \\cdot \\text{error\\_hidden}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Gradient of the Loss with Respect to $ b_1 $ (Hidden Layer Bias)**:\n",
    "The bias gradient for $ b_1 $ is the sum of the hidden layer error across all examples in the batch:\n",
    "$$\n",
    "db_1 = \\sum \\text{error\\_hidden}\n",
    "$$\n",
    "Averaged over the batch size $ m $:\n",
    "$$\n",
    "db_1 = \\frac{1}{m} \\sum \\text{error\\_hidden}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Update the Weights and Biases**:\n",
    "Finally, update the weights and biases using gradient descent:\n",
    "$$\n",
    "W_1 = W_1 - \\alpha dW_1\n",
    "$$\n",
    "$$\n",
    "b_1 = b_1 - \\alpha db_1\n",
    "$$\n",
    "$$\n",
    "W_2 = W_2 - \\alpha dW_2\n",
    "$$\n",
    "$$\n",
    "b_2 = b_2 - \\alpha db_2\n",
    "$$\n",
    "Where:\n",
    "- $ \\alpha $ is the learning rate.\n",
    "- $ dW_1, db_1, dW_2, db_2 $ are the gradients calculated earlier.\n",
    "\n",
    "# Why Divide by $ m $ in Backpropagation?\n",
    "\n",
    "When performing backpropagation and computing the gradients of the loss function, it’s important to divide the gradients by the number of samples in the batch, denoted as $m $. This is done to **average the gradients** over all the training examples in the batch.\n",
    "\n",
    "## Reasons for Dividing by $ m $:\n",
    "\n",
    "1. **Averaging the Gradient**:\n",
    "   - When working with mini-batch gradient descent or full-batch gradient descent, the loss is typically the sum of the losses for all the examples in the batch.\n",
    "   - To ensure the gradient reflects the **average loss per sample** rather than the total loss, the gradient is divided by $ m $, the batch size.\n",
    "   - This prevents the gradient from becoming too large when using larger batches and ensures that the step size (controlled by the learning rate) remains consistent, regardless of batch size.\n",
    "\n",
    "2. **Scaling with Respect to the Batch Size**:\n",
    "   - Without dividing by $ m $, the magnitude of the gradient would scale directly with the number of examples in the batch. This would cause larger batches to have larger gradient updates, which could destabilize training.\n",
    "   - By dividing by $ m $, the update becomes independent of the batch size, ensuring that the weight updates are more stable and consistent, regardless of how many samples are processed at once.\n",
    "\n",
    "3. **Maintaining Stability in Training**:\n",
    "   - If we didn’t divide by $ m $, the gradients would be much larger for large batches, and smaller for small batches, making it difficult to tune the learning rate.\n",
    "   - Dividing by $m $ helps **normalize** the gradient, so that the learning rate $ \\alpha $ can be used effectively without needing to be adjusted based on the batch size.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Consider a batch size $ m = 64 $. If the sum of the errors across the batch is used directly, the resulting gradient would be 64 times larger than if a batch size of 1 was used. By dividing the gradient by 64, we ensure that the weight update is scaled appropriately for the number of examples in the batch.\n",
    "\n",
    "---\n",
    "\n",
    "In summary, dividing by $ m $ ensures that:\n",
    "- The gradient reflects the average per-sample contribution to the loss.\n",
    "- The weight updates remain stable and are not affected by changes in batch size.\n",
    "- It becomes easier to tune and apply a consistent learning rate during training.\n",
    "\n",
    "---\n",
    "# Loss Tracking and Computing Loss During Training\n",
    "\n",
    "To track the performance of the model during training, we can compute the **cross-entropy loss** at the end of each epoch. This gives a measure of how well the predictions match the true labels, and tracking the loss over epochs helps monitor the learning progress.\n",
    "\n",
    "---\n",
    "\n",
    "## Cross-Entropy Loss Formula:\n",
    "\n",
    "For multi-class classification, the cross-entropy loss is computed as:\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ m $ is the number of training examples (batch size).\n",
    "- $ C $ is the number of classes.\n",
    "- $ y_{ij} $ is the true label for sample $ i $ and class $ j $ (`1` for the correct class, `0` for others).\n",
    "- $ \\hat{y}_{ij} $ is the predicted probability for sample $ i $ and class $ j $, which is the output from the softmax function.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps for Loss Tracking:\n",
    "\n",
    "1. **Forward Pass**:\n",
    "   - After each forward pass during training, compute the predictions $ \\hat{y} $ (output) using the softmax function.\n",
    "\n",
    "2. **Cross-Entropy Loss Calculation**:\n",
    "   - Compute the cross-entropy loss between the predicted output and the true labels $ y $.\n",
    "\n",
    "3. **Optional: Print Loss**:\n",
    "   - Print the loss every few epochs (e.g., every 100 epochs) to track the training progress.\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Calculation in Python:\n",
    "\n",
    "1. Compute the cross-entropy loss:\n",
    "   - Use the formula: \n",
    "   $$\n",
    "   \\text{loss} = -\\frac{1}{m} \\sum \\left[ y \\cdot \\log(\\hat{y}) \\right]\n",
    "   $$\n",
    "   Where $ y $ is the true labels and $ \\hat{y} $ is the predicted output from the softmax function.\n",
    "\n",
    "2. Print the loss every $n $ epochs (e.g., every 100 epochs) to monitor how the training progresses.\n",
    "\n",
    "## Example of Loss Tracking:\n",
    "\n",
    "- Inside the training loop, after the `forward` method:\n",
    "   - Compute the loss using the predicted output and the true labels.\n",
    "   - Print the loss every 100 epochs.\n",
    "\n",
    "This process will help visualize whether the model is improving its predictions over time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7b8b7b-d48b-403c-9683-58288b5ff89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3036117330186388\n",
      "Epoch 100, Loss: 2.3017806625581962\n",
      "Epoch 200, Loss: 2.3005702314823466\n",
      "Epoch 300, Loss: 2.299148613082996\n",
      "Epoch 400, Loss: 2.2973543057611088\n",
      "Epoch 500, Loss: 2.2950474620344625\n",
      "Epoch 600, Loss: 2.292079741808893\n",
      "Epoch 700, Loss: 2.288280459795769\n",
      "Epoch 800, Loss: 2.2834460954982663\n",
      "Epoch 900, Loss: 2.277345637159762\n",
      "Training Time: 317.43 seconds\n",
      "Accuracy: 22.88%\n"
     ]
    }
   ],
   "source": [
    "# Simple Neural Network with Sigmoid\n",
    "\n",
    "def time_training_numpy(nn, X_train, y_train, epochs):\n",
    "    import time\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the neural network\n",
    "    nn.train(X_train, y_train, epochs)\n",
    "    \n",
    "    # End timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate total training time\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    return training_time\n",
    "\n",
    "class SimpleNeuralNetworkSigmoid:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)*.01   #input_size x hidden_size\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)*.01   #hidden_size x output_size\n",
    "        self.b1= np.zeros((1,hidden_size))  #1 x hidden_size \n",
    "        self.b2= np.zeros((1,output_size))  #1 x output_size\n",
    "        self.learning_rate = learning_rate  # Set the learning rate\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z1 = X@self.W1+self.b1 #First Layer Calculation\n",
    "        self.A1=self.sigmoid(self.Z1) #sigmoid to first layer outputs\n",
    "        self.Z2 = self.A1@self.W2+self.b2 #Second layer calculation\n",
    "        self.A2 = self.softmax(self.Z2) #softmax to second layer\n",
    "        return self.A2\n",
    "\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "\n",
    "        self.m=X.shape[0]\n",
    "        \n",
    "        # Output layer error\n",
    "        self.error_output = output-y #output loss\n",
    "        self.dW2 = (np.transpose(self.A1)@self.error_output)/self.m #output layer loss\n",
    "        self.db2=np.sum(self.error_output, axis=0, keepdims=True)/self.m #output layer bias loss\n",
    "\n",
    "        # Hidden layer error\n",
    "        self.error_hidden = (self.error_output@np.transpose(self.W2))*self.sigmoid_derivative(self.A1) # Error for hidden layer neurons\n",
    "        self.dW1 = (np.transpose(X)@self.error_hidden)/self.m #weight error for hidden\n",
    "        self.db1 = np.sum(self.error_hidden,  axis=0, keepdims=True)/self.m #bias term error for hidden\n",
    "\n",
    "        #Update all the weights\n",
    "        self.W1 -=self.learning_rate*self.dW1\n",
    "        self.b1 -= self.learning_rate*self.db1\n",
    "        self.W2-=self.learning_rate*self.dW2\n",
    "        self.b2-=self.learning_rate*self.db2\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def train(self, X, y, epochs=1000):\n",
    "       \n",
    "        for epoch in range(epochs):\n",
    "        # Forward pass to get predictions\n",
    "            output = self.forward(X)\n",
    "        \n",
    "        # Backward pass to update weights\n",
    "            self.backward(X, y, output)\n",
    "        \n",
    "        # Compute the loss (cross-entropy)\n",
    "            loss = -np.mean(np.sum(y * np.log(output + 1e-8), axis=1))  # This calculates the loss, the small added term prevents log(0) issues\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "    \n",
    "            \n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "\n",
    "# Initialize the neural network\n",
    "nn = SimpleNeuralNetworkSigmoid(input_size=3072, hidden_size=64, output_size=10, learning_rate=0.01)\n",
    "\n",
    "# Time and train the neural network\n",
    "time_training_numpy(nn, X_train, y_train, epochs=1000)\n",
    "# Train the neural network\n",
    "#nn.train(X_train, y_train, epochs=1000)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nn.predict(X_test)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test_labels)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddadc857-ae9e-41e4-a1d0-7b738890ce00",
   "metadata": {},
   "source": [
    "## Simple Neural Network with ReLU Activation\n",
    "\n",
    "This neural network is a simple fully connected feedforward neural network with the following components:\n",
    "\n",
    "### 1. **Network Architecture**\n",
    "- **Input Layer**: The input to the network consists of the flattened pixel values of CIFAR-10 images. Since each image is 32x32 pixels with 3 color channels (RGB), the input size is $( 32 \\times 32 \\times 3 = 3072 $).\n",
    "- **Hidden Layer**: There is a hidden layer with 64 neurons. The activation function used in this layer is **ReLU (Rectified Linear Unit)**. ReLU introduces non-linearity and helps the network learn more complex patterns. \n",
    "  $$\n",
    "  \\text{ReLU}(z) = \\max(0, z)\n",
    "  $$\n",
    "- **Output Layer**: The output layer has 10 neurons, corresponding to the 10 classes of CIFAR-10. This layer uses the **softmax** function to output probabilities for each class. Softmax ensures that the output values sum to 1, allowing us to interpret them as probabilities.\n",
    "  $$\n",
    "  \\text{Softmax}(z) = \\frac{e^{z}}{\\sum_{j} e^{z_j}}\n",
    "  $$\n",
    "\n",
    "### 2. **Forward Pass**\n",
    "In the forward pass, the input data is passed through the layers of the network as follows:\n",
    "- **Input to Hidden Layer**: \n",
    "  $$\n",
    "  Z_1 = W_1 \\cdot X + b_1\n",
    "  $$\n",
    "  Where:\n",
    "  - $( W_1 $) is the weight matrix for the input to hidden layer.\n",
    "  - $( X $) is the input data.\n",
    "  - $( b_1 $) is the bias vector for the hidden layer.\n",
    "  - $( Z_1 $) is the pre-activation values for the hidden layer.\n",
    "  \n",
    "  Then, the **ReLU activation** is applied:\n",
    "  $$\n",
    "  A_1 = \\text{ReLU}(Z_1)\n",
    "  $$\n",
    "\n",
    "- **Hidden to Output Layer**:\n",
    "  $$\n",
    "  Z_2 = W_2 \\cdot A_1 + b_2\n",
    "  $$\n",
    "  Where:\n",
    "  - $( W_2 $) is the weight matrix for the hidden to output layer.\n",
    "  - $( A_1 $) is the output of the hidden layer after applying ReLU.\n",
    "  - $( b_2 $) is the bias vector for the output layer.\n",
    "  - $( Z_2 $) is the pre-activation values for the output layer.\n",
    "  \n",
    "  Then, the **softmax activation** is applied:\n",
    "  $$\n",
    "  A_2 = \\text{softmax}(Z_2)\n",
    "  $$\n",
    "\n",
    "### 3. **Loss Function**\n",
    "The model uses **cross-entropy loss** to measure how far the predicted probabilities are from the true class labels:\n",
    "$$\n",
    "\\text{Loss} = - \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{c=1}^{C} y_{i,c} \\cdot \\log(\\hat{y}_{i,c})\n",
    "$$\n",
    "Where:\n",
    "- $( m $) is the number of training samples.\n",
    "- $( C $) is the number of classes (10 for CIFAR-10).\n",
    "- $( y_{i,c} $) is the true label for sample $( i $) and class $( c $).\n",
    "- $( \\hat{y}_{i,c} $) is the predicted probability for sample $( i $) and class $( c $).\n",
    "\n",
    "### 4. **Backpropagation and Gradient Descent**\n",
    "During backpropagation, the network computes the gradients of the loss with respect to the weights and biases, and updates them using gradient descent:\n",
    "- **Output Layer Gradients**:\n",
    "  $$\n",
    "  W_2 = W_2 - \\text{learning rate} \\times \\frac{\\partial \\text{Loss}}{\\partial W_2}\n",
    "  $$\n",
    "  $$\n",
    "  b_2 = b_2 - \\text{learning rate} \\times \\frac{\\partial \\text{Loss}}{\\partial b_2}\n",
    "  $$\n",
    "- **Hidden Layer Gradients**:\n",
    "  $$\n",
    "  W_1 = W_1 - \\text{learning rate} \\times \\frac{\\partial \\text{Loss}}{\\partial W_1}\n",
    "  $$\n",
    "  $$\n",
    "  b_1 = b_1 - \\text{learning rate} \\times \\frac{\\partial \\text{Loss}}{\\partial b_1}\n",
    "  $$\n",
    "\n",
    "### 5. **Training**\n",
    "The network is trained over a number of **epochs**. In each epoch:\n",
    "1. The input data is passed through the network in the forward pass.\n",
    "2. The loss is calculated using cross-entropy.\n",
    "3. The gradients are computed in the backward pass.\n",
    "4. The weights and biases are updated using gradient descent.\n",
    "\n",
    "### 6. **Prediction and Accuracy**\n",
    "- After training, the network is evaluated on the test set. The model outputs class probabilities, and the class with the highest probability is selected as the predicted class.\n",
    "- **Accuracy** is calculated as the percentage of correctly classified images:\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Test Images}} \\times 100\n",
    "  $$\n",
    "\n",
    "### Key Changes in This Model:\n",
    "- **ReLU Activation**: ReLU is used in the hidden layer instead of sigmoid, allowing the network to handle more complex data and speed up learning by avoiding the vanishing gradient problem.\n",
    "- **Deeper Hidden Layer**: The hidden layer with 64 neurons allows the model to learn more abstract representations of the data.\n",
    "\n",
    "### Summary\n",
    "This updated neural network with **ReLU activation** and a **deeper hidden layer** improves learning by allowing the model to capture more complex patterns in the CIFAR-10 dataset, while also addressing issues like the vanishing gradient problem with sigmoid activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef7a0b-e79b-49ff-ba71-c921e58924ef",
   "metadata": {},
   "source": [
    "# Simple Neural Network with ReLU Activation\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{cccc}\n",
    "\\text{Input Layer} & \\longrightarrow & \\text{Hidden Layer} & \\longrightarrow & \\text{Output Layer} \\\\\n",
    "\\left(3072\\right)  &                  & \\left(64\\right)     &                  & \\left(10\\right) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3035f0d-5894-4ca0-85c2-0dd6d8c18006",
   "metadata": {},
   "source": [
    "# Rectified Linear Unit (ReLU) Activation Function\n",
    "\n",
    "The **Rectified Linear Unit (ReLU)** is one of the most commonly used activation functions in modern neural networks due to its simplicity and effectiveness.\n",
    "\n",
    "## ReLU Function\n",
    "\n",
    "The ReLU function is defined as:\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z)\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- If $ z > 0 $, the output is $ z $.\n",
    "- If $ z \\leq 0 $, the output is 0.\n",
    "\n",
    "ReLU introduces non-linearity to the model but is computationally efficient because it only involves comparing the input $ z $ to zero.\n",
    "\n",
    "#### ReLU Derivative\n",
    "\n",
    "The derivative of ReLU is useful for backpropagation and is defined as:\n",
    "$$\n",
    "\\frac{d}{dz} \\text{ReLU}(z) = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } z > 0 \\\\\n",
    "0 & \\text{if } z \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This means that the gradient is:\n",
    "- 1 for positive inputs $ z $.\n",
    "- 0 for negative inputs $z $ or exactly zero.\n",
    "\n",
    "#### Advantages of ReLU:\n",
    "1. **Non-Saturating**: Unlike sigmoid or tanh, ReLU doesn't suffer from saturation in the positive regime. This helps to mitigate the **vanishing gradient problem**.\n",
    "2. **Sparse Activation**: ReLU outputs zero for all negative inputs, making the network sparse, which can help with the efficiency of learning.\n",
    "3. **Efficient Computation**: ReLU is computationally simple (just a max operation) and fast to compute.\n",
    "\n",
    "#### Disadvantages of ReLU:\n",
    "1. **Dying ReLU Problem**: Sometimes, neurons can \"die\" during training because they output zero and never activate again, especially with poor weight initialization or high learning rates. This can cause gradients to be zero for some neurons, preventing them from updating their weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d5d4344-d738-42a7-bb4c-2809bae1b302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.302996183758482\n",
      "Epoch 100, Loss: 2.29281813239517\n",
      "Epoch 200, Loss: 2.2685755959105887\n",
      "Epoch 300, Loss: 2.218453245999388\n",
      "Epoch 400, Loss: 2.1554619275496685\n",
      "Epoch 500, Loss: 2.108303987880914\n",
      "Epoch 600, Loss: 2.0735175012896208\n",
      "Epoch 700, Loss: 2.0431460618427173\n",
      "Epoch 800, Loss: 2.015858378261373\n",
      "Epoch 900, Loss: 1.9921219009246585\n",
      "Training Time: 273.41 seconds\n",
      "Accuracy: 28.75%\n"
     ]
    }
   ],
   "source": [
    "# Simple Neural Network with Relu\n",
    "\n",
    "\n",
    "def time_training_numpy(nn, X_train, y_train, epochs):\n",
    "    import time\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the neural network\n",
    "    nn.train(X_train, y_train, epochs)\n",
    "    \n",
    "    # End timer\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate total training time\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    return training_time\n",
    "\n",
    "\n",
    "class SimpleNeuralNetworkRelu:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)*.01   #input_size x hidden_size\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)*.01   #hidden_size x output_size\n",
    "        self.b1= np.zeros((1,hidden_size))  #1 x hidden_size \n",
    "        self.b2= np.zeros((1,output_size))  #1 x output_size\n",
    "        self.learning_rate = learning_rate  # Set the learning rate\n",
    "\n",
    "    def reLU(self, z):\n",
    "        return np.maximum(0,z)\n",
    "\n",
    "    def reLU_derivative(self, z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z1 = X@self.W1+self.b1 #First Layer Calculation\n",
    "        self.A1=self.reLU(self.Z1) #reLU to first layer outputs\n",
    "        self.Z2 = self.A1@self.W2+self.b2 #Second layer calculation\n",
    "        self.A2 = self.softmax(self.Z2) #softmax to second layer\n",
    "        return self.A2\n",
    "\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "\n",
    "        \n",
    "        #Output layer error\n",
    "        self.m=X.shape[0]\n",
    "        self.error_output = output-y #output loss\n",
    "        self.dW2 = (np.transpose(self.A1)@self.error_output)/self.m #output layer loss\n",
    "        self.db2=np.sum(self.error_output, axis=0, keepdims=True)/self.m #output layer bias loss\n",
    "\n",
    "        #Hidden Layer Error\n",
    "        self.error_hidden = (self.error_output@np.transpose(self.W2))*self.reLU_derivative(self.A1) # Error for hidden layer neurons\n",
    "        self.dW1 = (np.transpose(X)@self.error_hidden)/self.m #weight error for hidden\n",
    "        self.db1 = np.sum(self.error_hidden,  axis=0, keepdims=True)/self.m #bias term error for hidden\n",
    "\n",
    "        #Update all the weights\n",
    "        self.W1 -=self.learning_rate*self.dW1\n",
    "        self.b1 -= self.learning_rate*self.db1\n",
    "        self.W2-=self.learning_rate*self.dW2\n",
    "        self.b2-=self.learning_rate*self.db2\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def train(self, X, y, epochs=1000):\n",
    "        m = X.shape[0]  # Defining the batch size within the method\n",
    "        for epoch in range(epochs):\n",
    "        # Forward pass to get predictions\n",
    "            output = self.forward(X)\n",
    "        \n",
    "        # Backward pass to update weights\n",
    "            self.backward(X, y, output)\n",
    "        \n",
    "        # Compute the loss (cross-entropy)\n",
    "            loss = -np.mean(np.sum(y * np.log(output + 1e-8), axis=1))  # This calculates the loss, the small added term prevents log(0) issues\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "    \n",
    "            \n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)\n",
    "\n",
    "\n",
    "# Initialize the neural network\n",
    "nn = SimpleNeuralNetworkRelu(input_size=3072, hidden_size=64, output_size=10, learning_rate=0.01)\n",
    "\n",
    "\n",
    "# Time and train the neural network\n",
    "time_training_numpy(nn, X_train, y_train, epochs=1000)\n",
    "\n",
    "# Train the neural network\n",
    "#nn.train(X_train, y_train, epochs=1000)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nn.predict(X_test)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_test_labels)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f94bf03-5582-4627-b03e-b9ea80a0da6b",
   "metadata": {},
   "source": [
    "# Cleanup code deletes the dataset after NN has run freeing up space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c3327ef-4651-4f6c-b383-c66ddda0153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted dataset directory: ./cifar-10-batches-py\n",
      "Deleted dataset tar file: cifar-10-python.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Cleanup step: delete dataset after NN has run\n",
    "def cleanup_cifar10():\n",
    "    data_dir = './cifar-10-batches-py'\n",
    "    tar_file = 'cifar-10-python.tar.gz'\n",
    "    \n",
    "    # Remove the extracted dataset directory\n",
    "    if os.path.exists(data_dir):\n",
    "        shutil.rmtree(data_dir)\n",
    "        print(f\"Deleted dataset directory: {data_dir}\")\n",
    "    \n",
    "    # Optionally, remove the downloaded tar.gz file as well\n",
    "    if os.path.exists(tar_file):\n",
    "        os.remove(tar_file)\n",
    "        print(f\"Deleted dataset tar file: {tar_file}\")\n",
    "\n",
    "# Call cleanup after the NN has run\n",
    "cleanup_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416ad51-1a28-4873-99c3-66fce16243de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
